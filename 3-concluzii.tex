\chapter{Training TODO: change name, kinda cringe}
\section{Reaching a static target}
\subsection{The Problem}
A common task for an AI in a game is to reach a a given destination. According to \cite{alonso2020deeplearningnavigation} the common way to achieve this goal is by using \emph{navigation meshes} (NavMeshes). These are represented as graphs, and their nodes represent surfaces that can be traversed. Afterwards, algorithms such as \emph{A*} can be used to find the fastest way for an agent to go from point A to point B. However, these \emph{NavMeshes} require to be \emph{baked} (TODO explicatie baked) in advance, so updating them in real time can prove to be a challenge depending on several factors such as:
\begin{itemize}
    \item the game engine used
    \item the complexity of the game environment
    \item the computational cost associated with recreating in real time these meshes
\end{itemize}

A proposed solution for this problem would be to use AI agents that have been trained using deep learning methods, in such a manner that they would be independent from changes to the environment.

\subsection{Implementing the solution}
\paragraph{}
First, the action space for the agent is defined: due to the fact that we only need to move the agent to a given position, the action space consists of moving the agent forward or backward, and rotating it. Because the agent is supposed to be controlled by a controller, its movement input is defined as a real number in the $[-1, 1]$ interval for both $X$ and $Y$ axes. However, a discrete action space will be used instead of a continuous one to reduce the difficulty of learning, and also because there is no need for the agent to have movements that are so precise. For each axis the action space will be represented by the discrete space: $\{-1, -0.5, 0, 0.5, 1\}$.

\paragraph{}
To make decisions, the agent will need to make several observations about its surroundings. Firstly, it will need to know how close it is to certain objects in the environment. To accomplish this, several \emph{raycasts} will be used to measure the distance from the agent, similar to a \emph{LIDAR} (TODO: add ceva despre lidar si un paper). The raycasts start from the center of the agent and are spread in such a way that the angle between 2 consecutive rays is equal for any 2 consecutive rays (TODO: imagine cu rays, si probabil sa gasesc un termen mai ok). The observation will contain the distance until the rays hit an object. Also, the index of the layer of the hit object will be included in the observations, so that the agent will differentiate between regular environment objects and more important objects, such as other players, enemies, etc. For this implementation, 32 rays were used.

Other observations that are made are the agent's position in space and also that of the target. This observation is included so that the agent can learn how movement brings it closer or further to the target. The next observation is the agent's forward vector so it can know in which direction it is moving. The optimal direction that the agent should take is also observed and obtained by computing the vector difference between the target's position and the agent's position. To know how much to adjust its trajectory, the angle between the agent's forward vector and the target is observed; the angle is signed so that the agent can learn to adjust its trajectory to the left or to the right. The distance to the target is added to the observations so that the agent can learn that when the distance is getting smaller it is rewarded. The agent's normalized velocity vector is observed to show in which direction it is moving based on the given input. The angle between the agent's velocity vector and its forward vector is observed to tell if the agent is moving forwards or backwards. Finally, the agent's velocity magnitude is observed so the agent can know if it is moving or standing still. 

In summary the following observations are being made:
\begin{itemize}
    \item distance for each raycast until it hits an object
    \item layer index of object hit by the raycast
    \item spatial postion of the agent
    \item spatial position of the target
    \item agent's forward vector
    \item optimal direction of the agent
    \item signed angle between the agent's forward vector and the target
    \item distance from the target
    \item agent's normalized velocity vector
    \item angle between the agent's velocity vector and its forward vector
    \item agent's velocity magnitude
\end{itemize}

\paragraph{}
TODO: Pedepse si recompense

\subsection{Training}

TODO:

Despre ce sa scriu:
- ce observatii am ales
- ce recompense si pedepse ii dau
- faptu ca initial nu a mers cand am bagat pozitiile
- am 30 de agenti care se antreneaza in paralel
- la training sa zic despre configuratiile folosite si sa fac grafice cu mean rewards
\section{Reaching a moving target}
\section{Shooting a moving target}