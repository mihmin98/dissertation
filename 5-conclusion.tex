\chapter{Conclusions}

% = ce am realizat in acest experiment
% - pt acest task a fost mai ok sa am o retea mai mica
% - daca as vrea sa continui experimentu ar trebui sa-l las sa se antreneze mai mult ca sa vad daca atunci devin viabile solutiile cu mai multe layere
% - sa gasesc alte recompense sau observatii

% - la ala tactic paote era mai bine sa ii fac o chestie cu line of sight
% - nu are trigger discipline

In this paper, several behaviours of an AI that could be used in video games were implemented and tested using reinforcement learning, specifically PPO algorithms. The behaviours ranged form more simple ones, such as reaching given coordinates to more complex ones such as fighting an enemy and using a smart approach to the fight. To achive this, several rewards and observations were implemented so that the agent could learn the desired behaviour. The agent training was done using multiple network configurations, with different number of hidden layers and number of units per layer, in some cases testing different kinds of observations, by adding or removing specific ones. For the basic tasks, such as reaching a given point, or simply chasing a moving target, the experiments were successful, managing to correctly learn these behaviours and actually providing an alternative to more classical ways of moving a non-player character in the game's world. It has been shown that for these simple tasks, smaller networks perform much better than the larger ones and have several advantages over them, such as faster training times and less resources used during gameplay. The more complex behaviours of fighting an enemy did not manage to be successfully implemented like the previous ones so that they could be used in a real game, with the learned behaviour being suboptimal, not consisting of an interesting experience for a human player. This could be due to a number of factors such as reward and penalty values not being balanced correctly, agents trained for a shorter amount of time than what would be necessary to successfully learn a more complex behaviour, and not training the agent to fight a tank, but a target that is unable to fight back.

For further work on this topic, to improve the obtained results for the behaviour of fighting an enemy, new observations and rewards should be implemented or existing ones should have their values modified, the agents should be trained for longer periods of time and by fighting against AI controlled enemies, and different hyperparameter values should be tried.